---
layout: distill
title: Llama2
date: 2023-10-17
description: 关于 Llama2 的原理和应用
tags: large-language-model
categories: AI
giscus_comments: false
related_posts: false
toc:
  sidebar: left
---

在过去的几年里，大型语言模型（LLMs）——具有数十亿个参数的自然语言处理（NLP）系统——已经展示了新的能力，如生成创造性的文本，解决数学定理，预测蛋白质结构，回答阅读理解问题，等等。

这些项目是人工智能可以大规模地为数十亿人提供重大潜在利益的明确例子。

Meta 的 Llama2，希望让这项技术得到更广泛的应用。

## Llama2 简介

## Llama2 生态概览

从服务器、移动硬件到云平台、初创公司和企业，基于 Llama 的整个生态系统的每一层都在迅速发展[^1]。

2023年初，Meta 发布 Llama 1。

此后，基于 Llama 的模型，通过 Hugging Face 下载量超过 3000 万次，其中仅在过去 30 天就超过 1000 万次。就像 PyTorch 一样，Llama 已经发展成为一个构建平台。

- 云使用：AWS、Google Cloud 和 Microsoft Azure 等主要平台已在其平台上采用了Llama 模型，并且 Llama 2 在云中的存在正在扩大。
- 创新者：创新者和初创公司正在使 Llama 成为其生成式人工智能产品创新的基础。数以万计的初创公司正在使用或评估 Llama 2，包括 Anyscale、Replicate、
Snowflake、LangSmith、Scale AI 等。
- 众源优化/开发者社区：开源社区真正拥抱了该模型，新的工具、部署库、模型评估方法，甚至 Llama 的“微型”版本都在开发中，以便将Llama 引入边缘设备和移动平台。此外，社区还扩展了 Llama 以支持更大的上下文窗口，增加了对其他语言的支持等等。
- 硬件支持：硬件社区已完全接受 Llama 作为关键模型架构。各大硬件平台AMD、
Intel、Nvidia、Google均通过软硬件优化提升了Llama 2的性能。


### Llama 技术概览

Llama 2 使用公开的在线数据进行预训练。然后通过使用监督微调创建 Llama-2-chat 的初始版本。接下来，Llama-2-chat 使用人类反馈强化学习 (Reinforcement Learning from Human Feedback, RLHF) 进行迭代细化，其中包括拒绝采样和近端策略优化 (proximal policy optimization, PPO)。

![RLHF](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/358632284_992608712083884_4541893832375347808_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=f537c7&_nc_ohc=r7X2R8R0xBMAX-h_KnK&_nc_ht=scontent-sea1-1.xx&oh=00_AfDlm86zzHOqFXbN0zZKl8V9E0KEbo8Lq3CGygu3m4ldOQ&oe=65347B95)

- Llama 2 模型在 2 万亿个 token 上训练，上下文长度是 Llama 1 的两倍。Llama-2-chat 模型还在超过 100 万个新的人类注释上训练。
- Llama 2 在许多外部基准测试中都优于其他开源语言模型，包括推理、编码、熟练程度和知识测试。
- Llama-2-chat 使用来自人类反馈的强化学习来确保安全性和有益性。


## 参考
[^1]: https://ai.meta.com/blog/llama-2-updates-connect-2023/
[^2]: https://ai.meta.com/resources/models-and-libraries/llama/
