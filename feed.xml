<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tsichiro.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tsichiro.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-18T15:04:52+00:00</updated><id>https://tsichiro.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Llama2</title><link href="https://tsichiro.github.io/blog/2023/llama2/" rel="alternate" type="text/html" title="Llama2"/><published>2023-10-17T00:00:00+00:00</published><updated>2023-10-17T00:00:00+00:00</updated><id>https://tsichiro.github.io/blog/2023/llama2</id><content type="html" xml:base="https://tsichiro.github.io/blog/2023/llama2/"><![CDATA[<p>Llama 是2023年以来最流行的开源大语言模型（LLM）之一。</p> <p>在过去的几年里，大语言模型——具有数十亿个参数的自然语言处理（NLP）系统——已经展示了新的能力，如生成创造性的文本，解决数学定理，预测蛋白质结构，回答阅读理解问题，等等。</p> <p>这些项目明确表明，人工智能有潜力在全球范围内为数十亿人提供重大的益处。</p> <h2 id="llama2-简介">Llama2 简介</h2> <h3 id="llama2-生态概览">Llama2 生态概览</h3> <p>Llama 开源且性能指标接近 chatGPT，从服务器、移动硬件到云平台、初创公司和企业，基于 Llama 的整个生态系统的每一层都在迅速发展<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p> <p>2023年初，Meta 发布 Llama 1。</p> <p>2023年中，Llama 2 发布，基于 Llama 的模型，通过 Hugging Face 下载量超过 3000 万次。</p> <p>就像 PyTorch 一样，Llama 已经发展成为一个开源、允许商用的构建平台。</p> <ul> <li>云使用：AWS、Google Cloud 和 Microsoft Azure 等主要平台已在其平台上采用了Llama 模型，并且 Llama 2 在云中的存在正在扩大。</li> <li>创新者：创新者和初创公司正在使 Llama 成为其生成式人工智能产品创新的基础。数以万计的初创公司正在使用或评估 Llama 2，包括 Anyscale、Replicate、 Snowflake、LangSmith、Scale AI 等。</li> <li>众源优化/开发者社区：开源社区真正拥抱了该模型，新的工具、部署库、模型评估方法，甚至 Llama 的“微型”版本都在开发中，以便将Llama 引入边缘设备和移动平台。此外，社区还扩展了 Llama 以支持更大的上下文窗口，增加了对其他语言的支持等等。</li> <li>硬件支持：硬件社区已完全接受 Llama 作为关键模型架构。各大硬件平台AMD、 Intel、Nvidia、Google均通过软硬件优化提升了Llama 2的性能。</li> </ul> <h3 id="llama-技术概览">Llama 技术概览</h3> <p>Llama 2 使用公开的在线数据进行预训练。然后通过使用监督微调创建 Llama-2-chat 的初始版本。接下来，Llama-2-chat 使用人类反馈强化学习 (Reinforcement Learning from Human Feedback, RLHF) 进行迭代细化，其中包括拒绝采样和近端策略优化 (proximal policy optimization, PPO)。</p> <p><img src="https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/358632284_992608712083884_4541893832375347808_n.jpg?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=f537c7&amp;_nc_ohc=r7X2R8R0xBMAX-h_KnK&amp;_nc_ht=scontent-sea1-1.xx&amp;oh=00_AfDlm86zzHOqFXbN0zZKl8V9E0KEbo8Lq3CGygu3m4ldOQ&amp;oe=65347B95" alt="RLHF"/></p> <ul> <li>Llama 2 模型在 2 万亿个 token 上训练，上下文长度是 Llama 1 的两倍。Llama-2-chat 模型还在超过 100 万个新的人类注释上训练。</li> <li>Llama 2 在许多外部基准测试中都优于其他开源语言模型，包括推理、编码、熟练程度和知识测试。</li> <li>Llama-2-chat 使用来自人类反馈的强化学习来确保安全性和有益性。</li> </ul> <h2 id="llama-的数学形式">Llama 的数学形式</h2> <p>《LLAMA-2 大语言模型的数学形式》<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">2</a></sup>，给出了该模型的数学形式。</p> <h2 id="参考">参考</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>https://ai.meta.com/blog/llama-2-updates-connect-2023/ <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>https://chinaxiv.org/abs/202308.00024 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="large-language-model"/><summary type="html"><![CDATA[关于 Llama2 的原理和应用]]></summary></entry></feed>